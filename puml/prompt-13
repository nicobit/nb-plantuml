📊 Prompt 1 EDA_Agent
1 Input files
• /data/train_clean.csv
• /data/val_clean.csv
• /data/test_clean.csv

2 Actions

What to do

Load all three files, parse the Date column, sort by date.

Profile each set: shape, columns, dtypes, missing percentage.

Compute mean, standard deviation, skewness, kurtosis, 1st & 99th percentiles for Open, High, Low, Close, Volume and Target_return.

Perform target diagnostics: Augmented Dickey-Fuller p-value on Target_return; 50-bin histogram; 20-day rolling volatility plot; 20-day “volatility-of-volatility” series.

Build a correlation heat-map for all OHLCV fields plus Target_return.

Flag outliers where absolute z-score > 3 for Volume or Target_return.

Derive and store:
lags_recommended, rolling_windows, clip_return_at, volume_skewed, strong_autocorr, vol_of_vol_regime_shift, weekday_drift.

Finish in ≤ 90 s using pandas, numpy, scipy, matplotlib, seaborn, statsmodels.

What NOT to do

Never modify the clean CSVs.

Avoid printing entire DataFrames.

Skip heavy tests beyond ADF.

Recommendations for a lower RMSE

Clip extreme Target_return values to ± 0.20.

Log-transform Volume if highly skewed.

Include Lag-1 Target_return when autocorrelation is strong.

Pass the regime-shift flag to downstream agents.

3 Summary artefacts
• EDA.py • eda_outputs/meta.json • eda_outputs/summary.txt
• eda_outputs/correlation.png • eda_outputs/target_hist.png • eda_outputs/rolling_vol.png

🛠 Prompt 2 FeatureEngineering_Agent
1 Input files
• /data/train_clean.csv, /data/val_clean.csv, /data/test_clean.csv
• eda_outputs/meta.json • eda_outputs/summary.txt

2 Actions

What to do

Ingest meta.json values.

Convert Date to index; keep ascending order.

Engineer leakage-safe features:
• Close lags for each recommended lag.
• Lag 1 of Target_return if strong_autocorr.
• Rolling and EWMA stats for Close & Volume (windows 5, 10, 20; spans 5 & 20).
• Percentage changes of Close & Volume.
• RSI-14, MACD diff, Bollinger-band width, ATR-14.
• Weekday & month one-hot dummies.
• VWAP proxy.
• Volume z-score and log1p(Volume) when volume_skewed is true.
• Interaction: lag_1_Close × RSI-14.
• Weekday-demeaned Target_return and five-day realised-vol z-score.
• sample_w column: 1.0 everywhere unless vol-of-vol indicates a regime shift, in which case up-weight recent rows.

Clip Target_return to ± clip_return_at.

Forward-fill then back-fill; drop residual NaNs.

Fit a min–max scaler on training predictors; reuse for val & test.

Keep identical column order in all *_features.csv files.

What NOT to do

No look-ahead in rolling or shifts.

Don’t fit separate scalers per split.

Remove predictors with > 30 % missing or near-zero variance.

Recommendations for a lower RMSE

Stay under ~150 predictors.

EWMA stats help in volatile regimes.

Preserve sample_w for the modelling stage.

3 Summary artefacts
• FEATURE.py • /data/train_features.csv, /data/val_features.csv, /data/test_features.csv
• eda_outputs/final_features.json • eda_outputs/scaler.json

🤖 Prompt 3 Modelling_Agent
1 Input files
• /data/train_features.csv
• /data/val_features.csv
• eda_outputs/final_features.json

2 Actions

What to do

Fix random seed to 42.

Split predictors (from final_features.json) and Target_return label.

Use sample_w from the feature files as observation weights.

Train two base models:
• LightGBM Regressor with 40-trial hyper-parameter search (time-series CV, early stopping).
• ExtraTrees Regressor with 600 trees.

Train a linear meta-model on validation predictions from both base models.

Report validation RMSE, MAE, R² once.

Save a dictionary bundle containing base models and the meta-model.

What NOT to do

No shuffled CV folds.

Don’t over-train; rely on early-stopping.

Avoid GPU-only libraries.

Recommendations for a lower RMSE

Combine slow-learn GBM with high-variance trees and stack linearly.

Learning rate ≤ 0.05 with many trees usually beats a higher rate.

Volatility-inverse weights focus loss where the metric cares most.

3 Summary artefacts
• MODEL.py • model.pkl (estimators + meta) • model_meta.json

🧪 Prompt 4 Evaluation_Agent
1 Input files
• model.pkl
• /data/test_features.csv
• eda_outputs/final_features.json

2 Actions

What to do

Load model.pkl; if it is not a dictionary, wrap it accordingly.

Align test predictors to final_features.json.

If a meta-model exists, feed it base-model predictions; else average with stored or equal weights.

Compute RMSE between predictions and true Target_return.

Output MSFT_Score.txt with “RMSE: x.xxxxx” (five decimals).

Create evaluation_log.json with timestamp, test-row count, RMSE and estimator keys.

Print RMSE once; finish in ≤ 30 s.

What NOT to do

Never modify test_features.csv.

No extra metrics or DataFrame dumps.

Always check the pickle type before indexing.

Recommendations for a lower RMSE

Verify predictor order and scaling match those used in training.

Optionally clip extreme predictions to ± clip_return_at.

3 Summary artefacts
• EVAL.py • MSFT_Score.txt • evaluation_log.json

------------------------------


─────────────────────────────────────────
🤖  PROMPT 3 — MODELLING_AGENT
─────────────────────────────────────────

1. INPUT FILES
   • /data/train_features.csv
   • /data/val_features.csv
   • eda_outputs/final_features.json   (ordered list of PREDICTOR columns only)

─────────────────────────────────────────
2. ACTIONS
─────────────────────────────────────────
WHAT TO DO
• Set a global random seed of 42.
• Load final_features.json → predictor_list.
• Read the training and validation CSVs.
  – Target column is “Target_return” (in the CSVs, NOT in predictor_list).
  – Build:
      X_train = train_df[predictor_list]
      y_train = train_df["Target_return"]
      X_val   = val_df[predictor_list]
      y_val   = val_df["Target_return"]
  – Extract sample_w from each DataFrame
      (use 1.0 everywhere if the column does not exist).

• Train two base models:
  1) LightGBMRegressor
     · Hyper-parameter search with 40 trials
       (Optuna if available, otherwise RandomizedSearchCV).
     · TimeSeriesSplit of 3 folds.
     · Early stopping after 50 rounds.
  2) ExtraTreesRegressor
     · 600 trees
     · max_features = "sqrt"
     · min_samples_leaf = 2
     · full depth, n_jobs = -1.

• Stack the base models:
  – Get validation predictions from both learners.
  – Fit an ordinary least-squares LinearRegression meta-model on
      [pred_lgbm , pred_etr]  →  y_val.

• Evaluate once on the validation set; print RMSE, MAE, and R².

• Save a bundle to model.pkl (via joblib):
      {
        "estimators": {"lgbm": <LightGBM>, "etr": <ExtraTrees>},
        "meta": <LinearRegression>
      }

• Create model_meta.json with:
    { "val_metrics": {...},
      "best_params": {...},
      "timestamp_utc": "<ISO-8601>" }

• Time limit ≤ 10 minutes; memory ≤ 4 GB.
  Allowed libraries: pandas, numpy, scikit-learn, lightgbm, joblib,
  optuna (optional).

─────────────────────────────────────────
WHAT NOT TO DO
• Do NOT shuffle rows in cross-validation: always preserve time order.
• Do NOT train until zero loss; rely on early stopping.
• Avoid GPU-only libraries or CUDA-dependent code.

─────────────────────────────────────────
RECOMMENDATIONS FOR LOWER RMSE
• Use learning_rate ≤ 0.05 with plenty of trees — usually beats higher rates.
• Stacking LightGBM (bias-strong) with ExtraTrees (variance-strong) plus
  a linear meta-model often cuts a few basis points off RMSE.
• Apply inverse-volatility sample weights (1/(|Target_return|+0.001)) to
  reduce the influence of crash days on training loss.

─────────────────────────────────────────
3. SUMMARY ARTEFACTS
   • MODEL.py           (self-running, self-debugging)
   • model.pkl          (dict with keys: estimators, meta)
   • model_meta.json
─────────────────────────────────────────
----------------------------


─────────────────────────────────────────────────────────────
WHAT TO DO  (revised – loop-safe)
─────────────────────────────────────────────────────────────
• Set a global random seed of 42.

• Load predictor_list from final_features.json.
  Target column is “Target_return” (not in that list).
  Build X_train, y_train, X_val, y_val accordingly.

• Drop any predictor whose training column has
    – zero variance  OR
    – the same value in > 99.9 % of rows.
  (This prevents LightGBM from cycling on constant features.)

• Extract sample_w from the data (default 1.0).

• Define a 40-trial hyper-parameter search for **LightGBM** with:
    – n_estimators (num_iterations) 250 → 1000
    – learning_rate  0.01 → 0.05
    – num_leaves     31 → 127
    – max_depth      -1, 5, 7
    – min_gain_to_split 0  or  0.001        ← NEW
    – min_data_in_leaf  20 → 100
    – bagging_fraction  0.8 or 1.0
    – feature_fraction  0.8 or 1.0
    – reg_lambda        0 → 5
  Early-stop after 50 rounds on a 3-fold TimeSeriesSplit.
  Put a **600-second wall-clock timeout** on the entire search; abort
  remaining trials if time is exceeded.

• If LightGBM training raises an exception OR returns 0 trees,
  **fallback** to a RandomForestRegressor with 600 trees,
  max_depth = None, max_features = "sqrt".

• Train an **ExtraTreesRegressor** with 600 trees
  (square-root features, min_samples_leaf = 2).

• Fit a linear meta-model on the two validation prediction vectors.

• Print validation RMSE, MAE, R² once.

• Save the bundle:
    {
      "estimators": {"lgbm_or_rf": <model_A>,
                     "etr": <extra_trees>},
      "meta": <linear_stack>
    }  →  model.pkl   (joblib)

• Write model_meta.json with metrics, best params, UTC timestamp.

• Stay within 10 min CPU and < 4 GB RAM.
  Allowed libraries: pandas, numpy, scikit-learn, lightgbm,
  joblib, optuna (optional).
─────────────────────────────────────────────────────────────



────────────────────────────────────────────────────────
🤖  PROMPT 3 — MODELLING_AGENT   (runtime-safe edition)
────────────────────────────────────────────────────────

1. INPUT FILES
   • /data/train_features.csv
   • /data/val_features.csv
   • eda_outputs/final_features.json      (predictor names only)

────────────────────────────────────────────────────────
2. ACTIONS
────────────────────────────────────────────────────────
WHAT TO DO
• Set `random_state = 42` globally.

• Load `final_features.json` → predictor_list.
  Target column is “Target_return” (present in the CSVs but NOT in predictor_list).

• Build:
    X_train = train_df[predictor_list]
    y_train = train_df["Target_return"]
    X_val   =  val_df[predictor_list]
    y_val   =  val_df["Target_return"]

• Extract `sample_w` from both data sets; if absent, create a 1.0 vector.

• **Pre-clean predictors**
  Drop any column that is constant or has the same value in > 99.9 % of rows.

• **LightGBM base learner**
  – Hyper-parameter search: 40 trials, TimeSeriesSplit(n_splits=3).  
  – Parameter bounds  
       num_iterations     : 250 – 800  
       learning_rate      : 0.01 – 0.05  
       num_leaves         : 31 – 127  
       max_depth          : −1 | 5 | 7  
       min_data_in_leaf   : 20 – 100  
       min_gain_to_split  : 0 | 0.001 | 0.005  
       max_bin            : 63 | 127  
       feature_fraction   : 0.8 | 1.0  
       bagging_fraction   : 0.8 | 1.0  
       bagging_freq       : 1  
       reg_lambda         : 0 – 5  
       objective          : “regression”  
       metric             : “rmse”  
  – **Early stopping**: pass `(X_val, y_val)` as the `eval_set` and set  
       `early_stopping_rounds = 50`.  
  – **Time guard**: abort the entire search after 600 s wall-clock.

• **Fallback**  
  If LightGBM errors or returns zero trees, fit a RandomForestRegressor
  (600 trees, max_features=“sqrt”, n_jobs=-1) using the same weights.

• **ExtraTrees secondary learner**
  – 600 trees, max_features=“sqrt”, min_samples_leaf=2, n_jobs=-1.

• **Linear meta-stack**
  – Collect validation predictions from both base models.  
  – Fit an ordinary least-squares LinearRegression on
      `[pred_lgbm_or_rf , pred_etr] → y_val`.

• **Validate once** on `y_val`; print RMSE, MAE, R² only once.

• **Save bundle** to model.pkl (joblib):
    {
      "estimators": {
          "primary"  : lgbm_or_rf,
          "secondary": extratrees
      },
      "meta": linear_meta
    }

• Write model_meta.json with:
    { "val_metrics": {...},
      "best_params": {...},
      "timestamp_utc": "<ISO-8601>" }

• Keep total wall-clock ≤ 10 minutes and memory ≤ 4 GB.
  Allowed libs: pandas, numpy, scikit-learn, lightgbm, joblib,
  optuna (optional).

────────────────────────────────────────────────────────
WHAT NOT TO DO
• ✗ Do NOT shuffle rows in cross-validation — preserve time order.
• ✗ Do NOT disable early stopping; it is mandatory.
• ✗ Avoid GPU-only libraries or CUDA-dependent flags.

────────────────────────────────────────────────────────
RECOMMENDATIONS FOR LOWER RMSE
• Low learning_rate with capped num_iterations balances speed and accuracy.
• Stacking a GBM (bias-strong) with ExtraTrees (variance-strong) plus a
  linear meta-model typically shaves several basis points off RMSE.
• Use inverse-volatility `sample_w` weights (1/(|Target_return|+0.001)) to
  concentrate loss where the evaluation metric is most sensitive.

────────────────────────────────────────────────────────
3. SUMMARY ARTEFACTS
   • MODEL.py        (self-running, self-debugging)
   • model.pkl       (dict: estimators + meta)
   • model_meta.json
────────────────────────────────────────────────────────

